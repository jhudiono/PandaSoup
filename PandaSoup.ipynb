{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PandaSoup Scraper Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse data from websites into a pandas DataFrame for each page.\n",
    "\n",
    "Start with a URL template, for example: \"http://www.google.com?q1={}&q2={}&q3={}\"\n",
    "\n",
    "Each time we perform a scrape, we iterate over the list of params to plug into the template:\n",
    "[\n",
    "    ('hi', 'stuff', '123'),\n",
    "    ('foo', '999', '2018/07/04'),\n",
    "    ...\n",
    "]\n",
    "\n",
    "Each of these tuples creates a new URL, which we scrape with BeautifulSoup. Results are saved in the \"raw_data\" dictionary as values under its parameter tuple as the key.\n",
    "\n",
    "Given an extract function (extract_func) to pull certain items out of each page of raw data to a dataframe, store in the list of dataframes. By default, the dict of pages is simply transformed to a dataframe with a 1-1 relationship, keeping the same key tuples.\n",
    "\n",
    "User can give a \"grouping strategy\" to consolidate data based on parameters. \n",
    "\n",
    "Results can be saved to disk as a CSV file per DataFrame object.\n",
    "\n",
    "Example: http://www.mywebsite.com?firstname={}&lastname={}&year={}&month={}&day={}, [\"year\", \"month\"]\n",
    "* By default, data entities will be separated for each parameter combo: myfile_firstname_lastname_year_month_day.csv\n",
    "* Using the strategy, they will be consolidated by year and month: myfile_year_month.csv\n",
    "\n",
    "## Constructor args:\n",
    "Pass a single dictionary. The following parameters are recognized.\n",
    "\n",
    "    params_dict = {\n",
    "        base : \"http://www.mywebsite.com?q={}\"\n",
    "        param_names : [keyword1, keyword2, keyword3...]\n",
    "        csv_base : \"my/data/path/{}_{}_{}.csv\"\n",
    "        request_delay : 123\n",
    "        extract_func: (lambda x: x)\n",
    "        grouping_strategy : [keyword1, keyword2... ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from __future__ import print_function\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "# TODO: Add selenium functionality later\n",
    "\n",
    "class PandaSoup:\n",
    "    raw_data = defaultdict(list)\n",
    "    request_delay = 5  # 5 seconds\n",
    "    data = defaultdict(list)\n",
    "    extract_func = (lambda x: x)\n",
    "    grouping_strategy = None\n",
    "    init_params = {}\n",
    "    use_timestamped_csv = True\n",
    "    \n",
    "    def __init__(self, params_dict):\n",
    "        self.base = params_dict.get('base', None)\n",
    "        self.param_names = params_dict.get('param_names', None)\n",
    "        self.grouping_strategy = params_dict.get('grouping_strategy', None)\n",
    "        # If user specified a custom csv, don't use timestamp in file names\n",
    "        if 'csv_base' in params_dict:\n",
    "            self.use_timestamped_csv = False\n",
    "            self.csv_base = params_dict['csv_base']\n",
    "        elif 'grouping_strategy' in params_dict:\n",
    "            self.csv_base = str.format(\"test/data/{}.csv\", \n",
    "                                       \"{}\"+(\"_{}\"*len(self.grouping_strategy)))\n",
    "        else:\n",
    "            self.csv_base = str.format(\"test/data/{}.csv\", \n",
    "                                       \"{}\"+(\"_{}\"*len(self.param_names)))\n",
    "        self.request_delay = params_dict.get('request_delay', 5)\n",
    "        self.extract_func = params_dict.get('extract_func', (lambda x: x))\n",
    "        self.init_params = params_dict\n",
    "        \n",
    "    def debug_str(self, level=1):\n",
    "        print(\"Raw data:\", len(self.raw_data), \"items\")\n",
    "        if level == 2:\n",
    "            print(\"    keys:\", self.raw_data.keys())\n",
    "        print(\"Data:\", len(self.data), \"items\")\n",
    "        print(\"Default output path:\", self.csv_base)\n",
    "        if level == 2:\n",
    "            print(\"    keys:\", self.data.keys())\n",
    "            print(\"Initialized with params:\", self.init_params)\n",
    "        \n",
    "    def scrape(self, params=[], verbose=None):\n",
    "        url = str.format(self.base, *params)\n",
    "        if verbose:\n",
    "            print(\"Reading\", url)\n",
    "        response = requests.get(url)\n",
    "        return BeautifulSoup(response.text,\"lxml\")\n",
    "        \n",
    "    def scrape_all(self, params=[], reset=False, verbose=None):\n",
    "        if reset:\n",
    "            self.raw_data = {}\n",
    "        for param_tuple in params:\n",
    "            key = param_tuple\n",
    "            if self.grouping_strategy:\n",
    "                key = tuple(\n",
    "                    [param_tuple[self.param_names.index(p)] for p in self.grouping_strategy]\n",
    "                )\n",
    "            if verbose:\n",
    "                print(\"parameter values\", param_tuple, \"--> key\", key)\n",
    "            self.raw_data[key].append(self.scrape(param_tuple, verbose))\n",
    "            time.sleep(self.request_delay)\n",
    "        return self.raw_data\n",
    "    \n",
    "    def make_dataframes(self, reset=False, verbose=None):\n",
    "        if reset:\n",
    "            print(\"Clear dataframes\")\n",
    "            self.data = {}\n",
    "        count = 0\n",
    "        for key,value in self.raw_data.items():\n",
    "            df = [self.extract_func(v) for v in value]\n",
    "            self.data[key] = pd.concat(df)\n",
    "            if verbose:\n",
    "                print(key, \"yielded\", len(df), \"rows of data\")\n",
    "        if verbose:\n",
    "            print(\"Completed extracting data\")\n",
    "        return self.data\n",
    "    \n",
    "    def to_csv(self, verbose=None):\n",
    "        files = []\n",
    "        for key,value in self.data.items():\n",
    "            file = self.csv_base \n",
    "            if self.use_timestamped_csv:\n",
    "                file = str.format(self.csv_base, math.floor(time.time()*10), *key)\n",
    "            else:\n",
    "                file = str.format(self.csv_base, *key)\n",
    "            if verbose:\n",
    "                print(\"Writing to\", file)\n",
    "            value.to_csv(file)\n",
    "            files.append(file)\n",
    "        return files\n",
    "        \n",
    "    def reset_all(self,verbose=None):\n",
    "        if verbose:\n",
    "            print(\"Clearing data\")\n",
    "        self.raw_data = {}\n",
    "        self.data = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test\n",
    "!mkdir test/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(soup):\n",
    "    players = {}\n",
    "    for n, link in enumerate(soup.find_all(href=re.compile('/stats/players'))):\n",
    "        name = link.text\n",
    "        if name != \"players\":\n",
    "            row = link.findParent().findParent()\n",
    "            players[n] = [td.text for td in row.find_all('td')]\n",
    "            players[n][0] = name\n",
    "    return pd.DataFrame(players).T\n",
    "\n",
    "test_params = {\n",
    "    'base' : \"http://fftoday.com/stats/playerstats.php?Season={}&GameWeek={}&PosID={}\",\n",
    "    'param_names' : [\"season\", \"week\", \"position\"],\n",
    "    'request_delay' : 3,\n",
    "    'extract_func': f,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter values (2015, 5, 10) --> key (2015, 5, 10)\n",
      "Reading http://fftoday.com/stats/playerstats.php?Season=2015&GameWeek=5&PosID=10\n",
      "parameter values (2016, 6, 10) --> key (2016, 6, 10)\n",
      "Reading http://fftoday.com/stats/playerstats.php?Season=2016&GameWeek=6&PosID=10\n",
      "(2015, 5, 10) yielded 2 rows of data\n",
      "(2016, 6, 10) yielded 2 rows of data\n",
      "Completed extracting data\n",
      "Writing to test/data/15315214257_2015_5_10.csv\n",
      "Writing to test/data/15315214257_2016_6_10.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['test/data/15315214257_2015_5_10.csv', 'test/data/15315214257_2016_6_10.csv']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default behavior\n",
    "twp = PandaSoup(test_params)\n",
    "twp.scrape_all([\n",
    "    (2015, 5, 10), \n",
    "    (2016, 6, 10)\n",
    "], verbose=True)\n",
    "twp.make_dataframes(verbose=True)\n",
    "twp.to_csv(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter values (2014, 3, 10) --> key (2014, 3)\n",
      "Reading http://fftoday.com/stats/playerstats.php?Season=2014&GameWeek=3&PosID=10\n",
      "parameter values (2015, 10, 10) --> key (2015, 10)\n",
      "Reading http://fftoday.com/stats/playerstats.php?Season=2015&GameWeek=10&PosID=10\n",
      "(2014, 3) yielded 1 rows of data\n",
      "(2015, 10) yielded 1 rows of data\n",
      "Completed extracting data\n",
      "Writing to test/data/15315209433_2014_3.csv\n",
      "Writing to test/data/15315209433_2015_10.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['test/data/15315209433_2014_3.csv', 'test/data/15315209433_2015_10.csv']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grouping strategy\n",
    "test_params_with_group = deepcopy(test_params)\n",
    "test_params_with_group['grouping_strategy'] = ['season', 'week']\n",
    "\n",
    "twp = PandaSoup(test_params_with_group)\n",
    "twp.scrape_all([\n",
    "    (2014, 3, 10), \n",
    "    (2015, 10, 10), \n",
    "], verbose=True)\n",
    "twp.make_dataframes(verbose=True)\n",
    "twp.to_csv(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data: 0 items\n",
      "    keys: dict_keys([])\n",
      "Data: 0 items\n",
      "Default output path: test/data/{}_{}_{}.csv\n",
      "    keys: dict_keys([])\n",
      "Initialized with params: {'base': 'http://fftoday.com/stats/playerstats.php?Season={}&GameWeek={}&PosID={}', 'param_names': ['season', 'week', 'position'], 'request_delay': 3, 'extract_func': <function f at 0x7f532be9bbf8>, 'csv_base': 'test/data/{}_{}_{}.csv'}\n",
      "parameter values (2015, 5, 10) --> key (2015, 5, 10)\n",
      "Reading http://fftoday.com/stats/playerstats.php?Season=2015&GameWeek=5&PosID=10\n",
      "parameter values (2016, 6, 10) --> key (2016, 6, 10)\n",
      "Reading http://fftoday.com/stats/playerstats.php?Season=2016&GameWeek=6&PosID=10\n",
      "(2015, 5, 10) yielded 1 rows of data\n",
      "(2016, 6, 10) yielded 1 rows of data\n",
      "Completed extracting data\n",
      "Writing to test/data/2015_5_10.csv\n",
      "Writing to test/data/2016_6_10.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['test/data/2015_5_10.csv', 'test/data/2016_6_10.csv']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom CSV\n",
    "test_params_with_csv = deepcopy(test_params)\n",
    "test_params_with_csv['csv_base'] = \"test/data/{}_{}_{}.csv\"\n",
    "\n",
    "twp = PandaSoup(test_params_with_csv)\n",
    "twp.debug_str(2)\n",
    "twp.scrape_all([\n",
    "    (2015, 5, 10), \n",
    "    (2016, 6, 10)\n",
    "], verbose=True)\n",
    "twp.make_dataframes(verbose=True)\n",
    "twp.to_csv(verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
