{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PandaSoup Scraper Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse data from websites into a pandas DataFrame for each page.\n",
    "\n",
    "Start with a URL template, for example: \"http://www.google.com?q1={}&q2={}&q3={}\"\n",
    "\n",
    "Each time we perform a scrape, we iterate over the list of params to plug into the template:\n",
    "[\n",
    "    ('hi', 'stuff', '123'),\n",
    "    ('foo', '999', '2018/07/04'),\n",
    "    ...\n",
    "]\n",
    "\n",
    "Each of these tuples creates a new URL, which we scrape with BeautifulSoup. Results are saved in the \"raw_data\" dictionary as values under its parameter tuple as the key.\n",
    "\n",
    "Given an extract function (extract_func) to pull certain items out of each page of raw data to a dataframe, store in the list of dataframes. By default, the dict of pages is simply transformed to a dataframe with a 1-1 relationship, keeping the same key tuples.\n",
    "\n",
    "User can give a \"grouping strategy\" to consolidate data based on parameters. \n",
    "\n",
    "Results can be saved to disk as a CSV file per DataFrame object.\n",
    "\n",
    "Example: http://www.mywebsite.com?firstname={}&lastname={}&year={}&month={}&day={}, [\"year\", \"month\"]\n",
    "* By default, data entities will be separated for each parameter combo: myfile_firstname_lastname_year_month_day.csv\n",
    "* Using the strategy, they will be consolidated by year and month: myfile_year_month.csv\n",
    "\n",
    "## Constructor args:\n",
    "Pass a single dictionary. The following parameters are recognized.\n",
    "\n",
    "    params_dict = {\n",
    "        base : \"http://www.mywebsite.com?q={}\"\n",
    "        param_names : [keyword1, keyword2, keyword3...]\n",
    "        csv_base : \"my/data/path/{}_{}_{}.csv\"\n",
    "        request_delay : 123\n",
    "        extract_func: (lambda x: x)\n",
    "        grouping_strategy : [keyword1, keyword2... ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from __future__ import print_function\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "# TODO: Add selenium functionality later\n",
    "\n",
    "class PandaSoup:\n",
    "    raw_data = defaultdict(list)\n",
    "    request_delay = 5  # 5 seconds\n",
    "    data = defaultdict(list)\n",
    "    extract_func = (lambda x: x)\n",
    "    grouping_strategy = None\n",
    "    init_params = {}\n",
    "    use_timestamped_csv = True\n",
    "    \n",
    "    def __init__(self, params_dict):\n",
    "        self.base = params_dict.get('base', None)\n",
    "        self.param_names = params_dict.get('param_names', None)\n",
    "        self.grouping_strategy = params_dict.get('grouping_strategy', None)\n",
    "        # If user specified a custom csv, don't use timestamp in file names\n",
    "        if 'csv_base' in params_dict:\n",
    "            self.use_timestamped_csv = False\n",
    "            self.csv_base = params_dict['csv_base']\n",
    "        elif 'grouping_strategy' in params_dict:\n",
    "            self.csv_base = str.format(\"test/data/{}.csv\", \n",
    "                                       \"{}\"+(\"_{}\"*len(self.grouping_strategy)))\n",
    "        else:\n",
    "            self.csv_base = str.format(\"test/data/{}.csv\", \n",
    "                                       \"{}\"+(\"_{}\"*len(self.param_names)))\n",
    "        self.request_delay = params_dict.get('request_delay', 5)\n",
    "        self.extract_func = params_dict.get('extract_func', (lambda x: x))\n",
    "        self.init_params = params_dict\n",
    "        \n",
    "    def debug_str(self, level=1):\n",
    "        print(\"Raw data:\", len(self.raw_data), \"items\")\n",
    "        if level == 2:\n",
    "            print(\"    keys:\", self.raw_data.keys())\n",
    "        print(\"Data:\", len(self.data), \"items\")\n",
    "        print(\"Default output path:\", self.csv_base)\n",
    "        if level == 2:\n",
    "            print(\"    keys:\", self.data.keys())\n",
    "            print(\"Initialized with params:\", self.init_params)\n",
    "        \n",
    "    def scrape(self, params=[], verbose=None):\n",
    "        url = str.format(self.base, *params)\n",
    "        if verbose:\n",
    "            print(\"Reading\", url)\n",
    "        response = requests.get(url)\n",
    "        return BeautifulSoup(response.text,\"lxml\")\n",
    "        \n",
    "    def scrape_all(self, params=[], reset=False, verbose=None):\n",
    "        if reset:\n",
    "            self.raw_data = {}\n",
    "        for param_tuple in params:\n",
    "            key = param_tuple\n",
    "            if self.grouping_strategy:\n",
    "                key = tuple(\n",
    "                    [param_tuple[self.param_names.index(p)] for p in self.grouping_strategy]\n",
    "                )\n",
    "            if verbose:\n",
    "                print(\"parameter values\", param_tuple, \"--> key\", key)\n",
    "            self.raw_data[key].append(self.scrape(param_tuple, verbose))\n",
    "            time.sleep(self.request_delay)\n",
    "        return self.raw_data\n",
    "    \n",
    "    def make_dataframes(self, reset=False, verbose=None):\n",
    "        if reset:\n",
    "            print(\"Clear dataframes\")\n",
    "            self.data = {}\n",
    "        count = 0\n",
    "        for key,value in self.raw_data.items():\n",
    "            df = [self.extract_func(v) for v in value]\n",
    "            self.data[key] = pd.concat(df)\n",
    "            if verbose:\n",
    "                print(key, \"yielded\", len(df), \"rows of data\")\n",
    "        if verbose:\n",
    "            print(\"Completed extracting data\")\n",
    "        return self.data\n",
    "    \n",
    "    def to_csv(self, verbose=None):\n",
    "        files = []\n",
    "        for key,value in self.data.items():\n",
    "            file = self.csv_base \n",
    "            if self.use_timestamped_csv:\n",
    "                file = str.format(self.csv_base, math.floor(time.time()*10), *key)\n",
    "            else:\n",
    "                file = str.format(self.csv_base, *key)\n",
    "            if verbose:\n",
    "                print(\"Writing to\", file)\n",
    "            value.to_csv(file)\n",
    "            files.append(file)\n",
    "        return files\n",
    "        \n",
    "    def reset_all(self,verbose=None):\n",
    "        if verbose:\n",
    "            print(\"Clearing data\")\n",
    "        self.raw_data = {}\n",
    "        self.data = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You imported PandaSoup!\n"
     ]
    }
   ],
   "source": [
    "print(\"You imported PandaSoup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
